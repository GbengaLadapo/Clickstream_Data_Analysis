from apache_beam.options.pipeline_options import PipelineOptions
from google.cloud import pubsub_v1
from google.cloud import bigquery
import apache_beam as beam
import logging
import argparse
import sys
import re
import configparser


def regex_clean(data):
    PATTERNS = [
        r'(^\S+\.[\S+\.]+\S+)\s',
        r'(?<=\[).+?(?=\])',
        r'\"(\S+)\s(\S+)\s*(\S*)\"',
        r'\s(\d+)\s',
        r"(?<=\[).\d+(?=\])",
        r'\"[A-Z][a-z]+',
        r'\"(http|https)://[a-z]+.[a-z]+.[a-z]+'
    ]
    result = []
    for match in PATTERNS:
        try:
            reg_match = re.search(match, data).group()
            if reg_match:
                result.append(reg_match)
            else:
                result.append(" ")
        except:
            print("There was an error with the regex search")
    result = [x.strip() for x in result]
    result = [x.replace('"', "") for x in result]
    res = ','.join(result)
    return res


class Split(beam.DoFn):
    def process(self, element):
        from datetime import datetime
        element = element.split(",")
        d = datetime.strptime(element[1], "%d/%b/%Y:%H:%M:%S")
        date_string = d.strftime("%Y-%m-%d %H:%M:%S")

        return [{
            'remote_addr': element[0],
            'timelocal': date_string,
            'request_type': element[2],
            'request_path': element[3],
            'status': element[4],
            'body_bytes_sent': element[5],
            'http_referer': element[6],
            'http_user_agent': element[7],
            'duration': element[8],
            'clicks': element[9],
            'purchase': element[10],
            'visit_type': element[11]
        }]


def main(argv=None):
    config = configparser.ConfigParser()
    config.read('config.cfg')
    schema = 'remote_addr:STRING, timelocal:STRING, request_type:STRING, request_path:STRING, status:STRING, body_bytes_sent:STRING, http_referer:STRING, http_user_agent:STRING, duration:STRING, clicks:STRING, purchase:STRING, visit_type:STRING'
    PROJECT_ID = config.get('PROJECT', 'PROJECT_ID')
    TOPIC_ID = config.get('TOPIC', 'TOPIC_ID')
    DATASET_ID = config.get('DATASET', 'DATASET_ID')

    parser = argparse.ArgumentParser()
    parser.add_argument("--input_topic", default=TOPIC_ID)
    parser.add_argument("--output", default=f"{PROJECT_ID}:{DATASET_ID}.clickstream")

    known_args = parser.parse_known_args(argv)

    p = beam.Pipeline(options=PipelineOptions())

    (p
     | 'ReadData' >> beam.io.ReadFromPubSub(topic=known_args[0].input_topic).with_output_types(bytes)
     | "Decode" >> beam.Map(lambda x: x.decode('utf-8'))
     | "Clean Data" >> beam.Map(regex_clean)
     | 'ParseCSV' >> beam.ParDo(Split())
     | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(known_args[0].output, schema=schema,
                                                     write_disposition=beam.io
